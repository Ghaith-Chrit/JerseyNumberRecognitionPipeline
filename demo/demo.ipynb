{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import *\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import helpers\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "import configuration_demo as config\n",
    "from matplotlib import pyplot as plt\n",
    "from data_augmentation import RandomSTRAugmentation\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\"dataset\": \"SoccerNet\", \"part\": \"test\", \"train_str\": False}\n",
    "args = SimpleNamespace(**args)\n",
    "\n",
    "os.makedirs(config.dataset[\"SoccerNet\"][\"working_dir\"], exist_ok=True)\n",
    "\n",
    "image_dir = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"root_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"images\"],\n",
    ")\n",
    "\n",
    "soccer_ball_list = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"soccer_ball_list\"],\n",
    ")\n",
    "\n",
    "features_dir = config.dataset[\"SoccerNet\"][args.part][\"feature_output_folder\"]\n",
    "full_legibile_path = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"legible_result\"],\n",
    ")\n",
    "\n",
    "illegible_path = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"illegible_result\"],\n",
    ")\n",
    "\n",
    "gt_path = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"root_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"gt\"],\n",
    ")\n",
    "\n",
    "input_json = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"pose_input_json\"],\n",
    ")\n",
    "\n",
    "output_json = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"pose_output_json\"],\n",
    ")\n",
    "\n",
    "crops_destination_dir = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"crops_folder\"],\n",
    "    \"imgs\",\n",
    ")\n",
    "\n",
    "str_result_file = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"jersey_id_result\"],\n",
    ")\n",
    "\n",
    "final_results_path = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"final_result\"],\n",
    ")\n",
    "\n",
    "crop_image_dir = os.path.join(\n",
    "    config.dataset[\"SoccerNet\"][\"working_dir\"],\n",
    "    config.dataset[\"SoccerNet\"][args.part][\"crops_folder\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legibility Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Determine soccer ball\")\n",
    "success = helpers.identify_soccer_balls(image_dir, soccer_ball_list)\n",
    "print(\"Done determine soccer ball\")\n",
    "\n",
    "print(\"\\nGenerate features\")\n",
    "command = f\"conda run -n {config.reid_env} --live-stream python {config.reid_script} --tracklets_folder {image_dir} --output_folder {features_dir}\"\n",
    "os.system(command)\n",
    "print(f\"Done generating features\")\n",
    "\n",
    "print(\"\\nIdentify and remove outliers\")\n",
    "command = f\"python gaussian_outliers.py --tracklets_folder {image_dir} --output_folder {features_dir}\"\n",
    "os.system(command)\n",
    "print(f\"Done removing outliers\")\n",
    "\n",
    "print(\"\\nClassifying Legibility:\")\n",
    "try:\n",
    "    legible_dict, illegible_tracklets = get_soccer_net_legibility_results(\n",
    "        args, use_filtered=True, filter=\"gauss\", exclude_balls=True\n",
    "    )\n",
    "except Exception as error:\n",
    "    print(f\"Failed to run legibility classifier:{error}\")\n",
    "    raise error\n",
    "print(\"Done classifying legibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Illegible Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(illegible_path, \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "image_paths = []\n",
    "directory_paths = data.get(\"illegible\", [])\n",
    "for directory in directory_paths:\n",
    "    dir_full_path = os.path.join(image_dir, directory)\n",
    "    for filename in os.listdir(dir_full_path):\n",
    "        if filename.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "            image_paths.append(os.path.join(dir_full_path, filename))\n",
    "\n",
    "images = []\n",
    "image_paths = image_paths[:5]\n",
    "for path in image_paths:\n",
    "    if os.path.exists(path):\n",
    "        img = cv2.imread(path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_resized = cv2.resize(img_rgb, (50, 128))\n",
    "        images.append(img_resized)\n",
    "    else:\n",
    "        print(f\"Image at path {path} not found!\")\n",
    "\n",
    "if len(images) == 0:\n",
    "    print(\"Failed to plot images: no illigble images exist\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(10, 5))\n",
    "\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        axes[i].imshow(images[i])\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Illegible Examples\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating json for pose\")\n",
    "try:\n",
    "    with open(full_legibile_path, \"r\") as openfile:\n",
    "        legible_dict = json.load(openfile)\n",
    "    generate_json_for_pose_estimator(args, legible=legible_dict)\n",
    "except Exception as error:\n",
    "    print(f\"Failed to run HPE: {error}\")\n",
    "    raise error\n",
    "print(\"Done generating json for pose\")\n",
    "\n",
    "print(\"\\nDetecting pose\")\n",
    "command = f\"conda run -n {config.pose_env} --live-stream python pose.py {config.pose_home}/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/ViTPose_huge_coco_256x192.py \\\n",
    "            {config.pose_home}/checkpoints/vitpose-h.pth --img-root / --json-file {input_json} \\\n",
    "            --out-json {output_json}\"\n",
    "os.system(command)\n",
    "print(\"Done detecting pose\")\n",
    "\n",
    "print(\"\\nGenerate crops\")\n",
    "try:\n",
    "    Path(crops_destination_dir).mkdir(parents=True, exist_ok=True)\n",
    "    with open(full_legibile_path, \"r\") as outfile:\n",
    "        legible_results = json.load(outfile)\n",
    "    helpers.generate_crops(output_json, crops_destination_dir, legible_results)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to run generate crops: {error}\")\n",
    "    raise error\n",
    "print(\"Done generating crops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for path in os.listdir(crops_destination_dir):\n",
    "    if len(images) > 5:\n",
    "        break\n",
    "    if not path.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "        continue\n",
    "    full_path = os.path.join(crops_destination_dir, path)\n",
    "    if os.path.exists(full_path):\n",
    "        img = cv2.imread(full_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        images.append(img_resized)\n",
    "    else:\n",
    "        print(f\"Image at path {full_path} not found!\")\n",
    "\n",
    "if len(images) == 0:\n",
    "    print(\"Failed to plot images: no cropped images exist\")\n",
    "else:\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(10, 5))\n",
    "\n",
    "    if len(images) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        axes[i].imshow(images[i])\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"Illegible Examples\", y=0.9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data Augmentation\n",
    "!!Not applied during test!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "input_data = crops_destination_dir\n",
    "for path in os.listdir(input_data):\n",
    "    if len(images) > 5:\n",
    "        break\n",
    "    if not path.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".gif\")):\n",
    "        continue\n",
    "    full_path = os.path.join(input_data, path)\n",
    "    if os.path.exists(full_path):\n",
    "        img = Image.open(full_path).convert(\"RGB\")\n",
    "        images.append(img)\n",
    "    else:\n",
    "        print(f\"Image at path {full_path} not found!\")\n",
    "\n",
    "augmented_images = []\n",
    "for img in images:\n",
    "    aug = RandomSTRAugmentation(mag_range=[0, 1])\n",
    "    augmented_images.append(aug(img))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(10, 5))\n",
    "\n",
    "if len(augmented_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(len(augmented_images)):\n",
    "    axes[i].imshow(augmented_images[i])\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"Augmented Examples\", y=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene Text Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predict numbers\")\n",
    "\n",
    "command = f\"conda run -n {config.str_env} --live-stream python str.py  {config.dataset['SoccerNet']['str_model']}\\\n",
    "\t--data_root={crop_image_dir} --batch_size=1 --inference --result_file {str_result_file}\"\n",
    "os.system(command)\n",
    "print(\"Done predict numbers\")\n",
    "\n",
    "analysis_results = None\n",
    "results_dict, analysis_results = helpers.process_jersey_id_predictions(\n",
    "    str_result_file, useBias=True\n",
    ")\n",
    "\n",
    "consolidated_dict = consolidated_results(\n",
    "    crop_image_dir, results_dict, illegible_path, soccer_ball_list=soccer_ball_list\n",
    ")\n",
    "\n",
    "with open(final_results_path, \"w\") as f:\n",
    "    json.dump(consolidated_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "with open(final_results_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "result = []\n",
    "for key in data.keys():\n",
    "    if len(result) == 3:\n",
    "        break\n",
    "    try:\n",
    "        key = int(key)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    dir_full_path = os.path.join(image_dir, str(key))\n",
    "    image_path = os.path.join(dir_full_path, random.choice(os.listdir(dir_full_path)))\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, (50, 128))\n",
    "    result.append((img_resized, data[str(key)]))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(result), figsize=(6, 5))\n",
    "\n",
    "if len(result) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i in range(len(result)):\n",
    "    axes[i].imshow(result[i][0])\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Predicted {result[i][1]}\")\n",
    "\n",
    "fig.suptitle(\"Predicted Resulls\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jerseySOTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
